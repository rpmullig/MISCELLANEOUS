{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpmullig/MISCELLANEOUS/blob/master/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtC1Y6ZiP6vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QgzsCnlliNXS"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "- Deadline: 11:59 pm, Monday, July 13th, 2020\n",
        "- Name: Robert Mulligan\n",
        "\n",
        "In this programming assignment, you will work on three tasks:  (1) sentiment analysis using logistic regression with bag of words, (2) sentiment analysis with word embeddings, and (3) and language modeling with RNNs.  To help you quickly get started most of the required code has already been provided.  You primary task is to undertand the provided code and fill in the gaps.\n",
        "\n",
        "You will use PyTorch extensively.  We recommend reading the first three tutorials on \"Deep Learning for NLP with PyTorch\" by Robert Guthrie https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html for this assignment.  Other tutorials are also useful, and they are mentioned later in this notebook.\n",
        "\n",
        "You should also seek our help unhesitatingly.  We want you to learn a lot of material in a short period of time, and our help will make it easier.  Please post your questions on Piazza or meet with us during office hours.\n",
        "\n",
        "### Sentiment Analysis\n",
        "\n",
        "For sentiment analysis you will work on large movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/). The task is to classify movie reviews into two categories, POSITIVE or NEGATIVE. \n",
        "\n",
        "You are provided with a training set (TRAIN), a development set (DEV), and a test set (TEST). Your classifier is trained on TRAIN, evaluated and tuned on DEV, and tested on TEST. \n",
        "\n",
        "You will build two classifiers in this homework, a logistic regression classifier with bag of words features and a neural network classifier with word embeddings.  For the logistic regression classifier with bag of words, we will preprocess the data from scratch. For the neural network based classifier with word embeddings, we will use torchtext for preprocessing the data. \n",
        "\n",
        "### Language modeling\n",
        "\n",
        "The last problem is on language modelling.  It may take a lot of time on a CPU. So, you can try to run it on Google collab.  Go to https://colab.research.google.com and log in using your Google account.  To upload a python notebook, click on \"Files\" dropdown menu and the upload notebook. To use GPU click the “Runtime” dropdown menu. Select “Change runtime type”. Select python2 or 3 from “Runtime type” dropdown menu and choose hardware accelerator as GPU. You can find detailed instructions on how to use google collab on this webpage https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVQnA3_6iNXc",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.utils.data as tud\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import os, math\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "# from nltk import word_tokenize\n",
        "\n",
        "def word_tokenize(s):\n",
        "    return s.split()\n",
        "\n",
        "# set the random seeds so the experiments can be replicated exactly\n",
        "random.seed(53113)\n",
        "np.random.seed(53113)\n",
        "torch.manual_seed(53113)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(53113)\n",
        "\n",
        "# Global class labels.\n",
        "POS_LABEL = 'pos'\n",
        "NEG_LABEL = 'neg'     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-d5yfrTaiNXv",
        "colab": {}
      },
      "source": [
        "def load_data(data_file):\n",
        "    data = []\n",
        "    with open(data_file,'r', encoding= \"Latin-1\") as fin:\n",
        "        for line in fin:\n",
        "            label, content = line.split(\",\", 1)\n",
        "            data.append((content.lower(), label))\n",
        "    return data\n",
        "data_dir = \"/content/drive/My Drive/large_movie_review_dataset\" # adjusted for Google Drive (mounted)\n",
        "train_data = load_data(os.path.join(data_dir, \"train.txt\"))\n",
        "dev_data = load_data(os.path.join(data_dir, \"dev.txt\"))\n",
        "\n",
        "def load_test_data(data_file):\n",
        "    data = []\n",
        "    with open(data_file,'r', encoding= \"Latin-1\") as fin:\n",
        "        for line in fin:\n",
        "            data.append(line.strip())\n",
        "    return data\n",
        "\n",
        "test_data = load_test_data(os.path.join(data_dir, \"test.txt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T6BlH7U-iNX8",
        "colab": {}
      },
      "source": [
        "print(\"number of TRAIN data\", len(train_data))\n",
        "print(\"number of DEV data\", len(dev_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Sbm751RiNYN"
      },
      "source": [
        "We define a generic model class as below. The model has 2 functions, train and classify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NFRfRpaiNYR",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 5000\n",
        "class Model:\n",
        "    def __init__(self, data):\n",
        "        # Vocabulary is a set that stores every word seen in the training data\n",
        "        self.vocab = Counter([word for content, label in data for word in word_tokenize(content)]).most_common(VOCAB_SIZE-1) \n",
        "        self.word_to_idx = {k[0]: v+1 for v, k in enumerate(self.vocab)} # word to index mapping\n",
        "        self.word_to_idx[\"UNK\"] = 0 # all the unknown words will be mapped to index 0\n",
        "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
        "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
        "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
        "        self.vocab = set(self.word_to_idx.keys())\n",
        "        \n",
        "    def train_model(self, data):\n",
        "        '''\n",
        "        Train the model with the provided training data\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def classify(self, data):\n",
        "        '''\n",
        "        classify the documents with the model\n",
        "        '''\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KnhR3qF_iNYc"
      },
      "source": [
        "## Sentiment Analysis with Logistic Regression and Bag of Words\n",
        "\n",
        "You will implement logistic regression with bag of words features in the following. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LTwWPNy2iNYg",
        "colab": {}
      },
      "source": [
        "class TextClassificationDataset(tud.Dataset):\n",
        "    '''\n",
        "    PyTorch provide a common dataset interface. \n",
        "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    The dataset encodes documents into indices. \n",
        "    With the PyTorch dataloader, you can easily get batched data for training and evaluation. \n",
        "    '''\n",
        "    def __init__(self, word_to_idx, data):\n",
        "        \n",
        "        self.data = data\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
        "        self.vocab_size = VOCAB_SIZE\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = np.zeros(self.vocab_size)\n",
        "        \n",
        "        item = torch.from_numpy(item)\n",
        "        if len(self.data[idx]) == 2: # in training or evaluation, we have both the document and label\n",
        "            for word in word_tokenize(self.data[idx][0]):\n",
        "                item[self.word_to_idx.get(word, 0)] += 1\n",
        "            label = self.label_to_idx[self.data[idx][1]]\n",
        "            return item, label\n",
        "        else: # in testing, we only have the document without label\n",
        "            for word in word_tokenize(self.data[idx]):\n",
        "                item[self.word_to_idx.get(word, 0)] += 1\n",
        "            return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tr-0xQveiNYn",
        "colab": {}
      },
      "source": [
        "best_model = None\n",
        "class BoWLRClassifier(nn.Module, Model):\n",
        "    '''\n",
        "    Define your logistic regression model with bag of words features.\n",
        "    '''\n",
        "    def __init__(self, data):\n",
        "        nn.Module.__init__(self)\n",
        "        Model.__init__(self, data)\n",
        "        \n",
        "        '''\n",
        "        In this model initialization phase, you will do the following: \n",
        "        1. Define a linear layer to transform bag of words features into 2 classes. \n",
        "        2. Define the loss function, you will use cross entropy loss\n",
        "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss\n",
        "        3. Define an optimizer for the model, you may choose to use SGD, Adam or other optimizers you know\n",
        "            https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD\n",
        "        '''\n",
        "        # added\n",
        "        self.linear = nn.Linear(VOCAB_SIZE, 2) \n",
        "        self.loss_fn = nn.CrossEntropyLoss() \n",
        "        self.optimizer = torch.optim.SGD(params=self.parameters(), lr=0.001) \n",
        "        \n",
        "        \n",
        "    def forward(self, bow):\n",
        "        '''\n",
        "        Run the model. You may only need to run the linear layer defined in the init function. \n",
        "        '''\n",
        "        out = self.linear(bow) \n",
        "        return out \n",
        "    \n",
        "    def train_epoch(self, train_data):\n",
        "        '''\n",
        "        Train the model for one epoch with the training data\n",
        "        When training a model, you will repeat the following procedures:\n",
        "        1. get one batch of features and labels\n",
        "        2. make a forward pass with the features to get predictions\n",
        "        3. calculate the loss with the predictions and target labels\n",
        "        4. run a backward pass from the loss function to get the gradients\n",
        "        5. apply the optimizer step to update the model paramters\n",
        "        '''\n",
        "        dataset = TextClassificationDataset(self.word_to_idx, train_data)\n",
        "        dataloader = tud.DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "        self.train()\n",
        "        for i, (X, y) in enumerate(dataloader):\n",
        "            X = X.float()\n",
        "            y = y.long()\n",
        "            if torch.cuda.is_available():\n",
        "                X = X.cuda()\n",
        "                y = y.cuda()\n",
        "            self.optimizer.zero_grad()\n",
        "            preds = self.forward(X)\n",
        "            loss = self.loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            if i % 500 == 0:\n",
        "                print(\"loss: {}\".format(loss.item()))\n",
        "            self.optimizer.step()\n",
        "    \n",
        "    def train_model(self, train_data, dev_data):\n",
        "        \"\"\"\n",
        "        This function processes the entire training set for multiple epochs.\n",
        "        After each training epoch, you will evaluate your model on the DEV set. \n",
        "        The best performing model on the DEV set shall be saved to best_model\n",
        "        \"\"\"  \n",
        "        dev_accs = [0.]\n",
        "        for epoch in range(2): # increase Epochs, results have not been helpful\n",
        "            self.train_epoch(train_data)\n",
        "            dev_acc = self.evaluate(dev_data)\n",
        "            print(\"dev acc: {}\".format(dev_acc))\n",
        "            if dev_acc > max(dev_accs):\n",
        "                best_model = copy.deepcopy(self)\n",
        "            dev_accs.append(dev_acc)\n",
        "\n",
        "    def classify(self, docs):\n",
        "        '''\n",
        "        This function classifies documents into their categories. \n",
        "        docs are documents only, without labels.\n",
        "        '''\n",
        "        dataset = TextClassificationDataset(self.word_to_idx, docs)\n",
        "        dataloader = tud.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "        results = []\n",
        "        with torch.no_grad():\n",
        "            for i, X in enumerate(dataloader):\n",
        "                X = X.float()\n",
        "                if torch.cuda.is_available():\n",
        "                    X = X.cuda()\n",
        "                preds = self.forward(X)\n",
        "                results.append(preds.max(1)[1].cpu().numpy().reshape(-1))\n",
        "        results = np.concatenate(results)\n",
        "        results = [self.idx_to_label[p] for p in results]\n",
        "        return results\n",
        "                \n",
        "    def evaluate(self, data):\n",
        "        '''\n",
        "        This function evaluate the data with the current model. \n",
        "        data contains documents and labels. \n",
        "        It calls function \"classify\" to make predictions, \n",
        "        and compare with the correct labels to return the model accuracy on \"data\". \n",
        "        '''\n",
        "        self.eval()\n",
        "        preds = self.classify([d[0] for d in data])\n",
        "        targets = [d[1] for d in data]\n",
        "        correct = 0.\n",
        "        total = 0.\n",
        "        for p, t in zip(preds, targets):\n",
        "            if p == t: \n",
        "                correct += 1\n",
        "            total += 1\n",
        "        return correct/total\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8GIZTEyjiNY0",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "lr_model = BoWLRClassifier(train_data)\n",
        "if torch.cuda.is_available():\n",
        "    lr_model = lr_model.cuda()\n",
        "lr_model.train_model(train_data, dev_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FAG7z8ZjiNZA"
      },
      "source": [
        "Now spend some time to tune your models. At least try the following: \n",
        "\n",
        "- try another optimizer\n",
        "- change the learning rate\n",
        "- change the number of epochs to train\n",
        "\n",
        "Report your results and analysis in the writeup. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puWYWLLh7l2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Increasing the epochs and Learning rate did not see significant improvement\n",
        "# But using Adam optimizer resulted in a 10% accuracy improvement\n",
        "lr_model.optimizer = optim.Adam(params=lr_model.parameters(), lr=0.001)\n",
        "lr_model.train_model(train_data, dev_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dkVBlI2giNZD",
        "colab": {}
      },
      "source": [
        "preds = lr_model.classify(test_data)\n",
        "def write_to_file(preds, filename):\n",
        "    i = 0\n",
        "    with open(os.path.join(filename), \"w\") as fout:\n",
        "        fout.write(\"index,label\\n\")\n",
        "        for pred in preds:\n",
        "            fout.write(\"{},{}\\n\".format(i, pred))\n",
        "            i += 1\n",
        "\n",
        "write_to_file(preds, \"lr_test_preds.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FcI52RIW8pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the weights from the linear layer \n",
        "weights = lr_model.linear.weight\n",
        "model_dataset = lr_model.word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eo-uTrWliNZM"
      },
      "source": [
        "Identify the top 10 features with the maximum weights for POSITIVE category. Explain your findings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uhx_NE-SiNZO",
        "colab": {}
      },
      "source": [
        "pos_values, pos_indices = torch.topk(input=weights[0], k=10, largest=True) # Positive\n",
        "\n",
        "i = 1\n",
        "for val, indx in model_dataset.items():       # O(m) search of dictionary (size of table = m) \n",
        "  if indx in pos_indices:                     # linear search of 10 elements\n",
        "    print(f\"#{i} \\\"{val}\\\" at index {indx}\")\n",
        "    i += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjEJlWupfBRq",
        "colab_type": "text"
      },
      "source": [
        "These are the most positive words. As you can tell, they make sense that they're helpful. For a good rating "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pk7fZ5vQiNZW"
      },
      "source": [
        "Identify the top 10 features with the maximum negative weights for POSITIVE category. Explain your findings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tWfPmouZiNZX",
        "colab": {}
      },
      "source": [
        "pos_values, pos_indices = torch.topk(input=weights[0], k=10, largest=False) # Positive smallest\n",
        "\n",
        "i = 1\n",
        "for val, indx in model_dataset.items():       # O(m) search of dictionary (size of table = m) \n",
        "  if indx in pos_indices:                     # linear search of 10 elements\n",
        "    print(f\"#{i} \\\"{val}\\\" at index {indx}\")\n",
        "    i += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trT2dRXoiNZp"
      },
      "source": [
        "Identify the top 10 features with the maximum positive weights for NEGATIVE category. Explain your findings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L6YqQeESiNZu",
        "colab": {}
      },
      "source": [
        "neg_values, neg_indices = torch.topk(input=weights[1], k=10, largest=True) # Negative \n",
        "\n",
        "i = 1\n",
        "for val, indx in model_dataset.items():       # O(m) search of dictionary (size of table = m) \n",
        "  if indx in neg_indices:                     # linear search of 10 elements\n",
        "    print(f\"#{i} \\\"{val}\\\" at index {indx}\")\n",
        "    i += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYfn8VOfiNZ1"
      },
      "source": [
        "Identify the top 10 features with the maximum negative weights for NEGATIVE category. Explain your findings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VGfwJdT4iNZ3",
        "colab": {}
      },
      "source": [
        "neg_values, neg_indices = torch.topk(input=weights[1], k=10, largest=False) # Negative smallest\n",
        "\n",
        "i = 1\n",
        "for val, indx in model_dataset.items():       # O(m) search of dictionary (size of table = m) \n",
        "  if indx in neg_indices:                     # linear search of 10 elements\n",
        "    print(f\"#{i} \\\"{val}\\\" at index {indx}\")\n",
        "    i += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8ktyoB9iNaB"
      },
      "source": [
        "## Sentiment Analysis with Word-Embeddings and a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "-cJuP5CtiNaC"
      },
      "source": [
        "We will use [torchtext](https://github.com/pytorch/text) to create vocabulary, and load datasets into batches. Please refer to their GitHub README page for a quick tutorial.  More details are in the set of tutorials at https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2cnpmGXiNaD",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# We are using 'spacy' tokenizer. You can also write your own tokenizer. You can download spacy from\n",
        "# this site https://spacy.io/usage\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "nOdI97jqiNaH"
      },
      "source": [
        "We know download the IMDB dataser=t using torchtext datasets. This step may take some 10 to 15 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ml75oPv6iNaJ",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C8g5-uUbiNaQ",
        "colab": {}
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "print(f'Number of validation examples:{len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iHAtG48oiNaW"
      },
      "source": [
        "Now we build our vocabulary/dictionary. We are only keeping 25,000 most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JFFWaYsziNaX",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sxLQezmKiNad"
      },
      "source": [
        "One can see vocabulary as well as the indices directly using either the stoi (string to int) or itos (int to string) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "INX-p_F5iNae",
        "colab": {}
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xoxAGZzaiNai"
      },
      "source": [
        "Next step is to build an iterator. This iterator will return a batch of data every iteration. It also pads every sentence to make all sentences in a batch of equal length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wn-__tIFiNak",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wiUnHqHniNan"
      },
      "source": [
        "The function binary_accuracy will be used to compute accuracy from logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6thdhypiNao",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJ6zV6SsiNar"
      },
      "source": [
        "In Class WordEmbAvg we define our model. Our model works in the following way.\n",
        "\n",
        "1. The input to our model is a batch of sentences. All sentences are made of equal length by padding.\n",
        "   Every word in the sentence is represented by one-hot encoding. So, sentence is a list of one-hot encoding.\n",
        "\n",
        "2. In input passes through an embedding layer. The embedding layer converts the one-hot encoding for every word into a word vector.\n",
        "\n",
        "3. We take the average of all the word vectors in a sentence. This vector is then used as an inout to a neural network.\n",
        "\n",
        "4. The neural network has only one output. It tells you the probability of the sentence having a particular label. (We will only have two labels)\n",
        "\n",
        "Please read about embedding from  https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJMPKhiS-X36",
        "colab_type": "text"
      },
      "source": [
        "#3.a\n",
        "Make $v(\\cdot)$ the one-hot encoding function of a word $$v(w_{i}) = \\{0, 1\\}$$\n",
        "\n",
        "Let $c$ be a sequence of words\n",
        "$$c = w_{i:k} = w_i, \\dots w_k$$\n",
        "\n",
        "Let $x$ be a vector of one-hot encodings $[v(w_i), v(w_{i+1}), \\dots v(w_k)]$\n",
        "\n",
        "Let $emb(w_i)$ is converting $v(w_i)$ to a word vector\n",
        "\n",
        "So, $X = [emb(v(w_i)), emb(v(w_{i+1})), \\dots emb(v(w_k))]$\n",
        "\n",
        "\n",
        "$$WordEmbAvg = \\frac{\\sum(emb(v(w_{i:|V|})))}{k}$$ \n",
        "\n",
        "$$h = softmax(WordEmbAvg*w_i + bias_{i})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWyqWHbTiNat",
        "colab": {}
      },
      "source": [
        "class WordEmbAvg(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, output_dim, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Define embedding layer in the next layer.\n",
        "        # It should be something like \n",
        "        #emb = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "        \n",
        "        self.emb = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "        \n",
        "        #Define your neural network. It can single layer or multiple layer neural network\n",
        "        # You don't need apply a softmax in the output layer\n",
        "        \n",
        "        # single linear layer\n",
        "        self.linear = nn.Linear(embedding_dim, output_dim) \n",
        "        \n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        \n",
        "        #Input goes to the embedding layer\n",
        "       \n",
        "        output = self.emb(text)\n",
        "        \n",
        "        # Take the average of all word embeddngs. Please check how to mean() on a tensor on pytorch\n",
        "        \n",
        "        output = torch.mean(output, 0)\n",
        "        \n",
        "        # Previous input now goes into the neural network\n",
        "        \n",
        "        output = self.linear(output)\n",
        "         \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YD1Y0axriNa3",
        "colab": {}
      },
      "source": [
        "class Training_module( ):\n",
        "\n",
        "    def __init__(self, model):\n",
        "       self.model = model\n",
        "    \n",
        "       #The loss function should be binary cross entropy with logits. \n",
        "       self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "       # Choose your favorite optimizer\n",
        "       self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "    \n",
        "    def train_epoch(self, iterator):\n",
        "        '''\n",
        "        Train the model for one epoch with the training data\n",
        "        When training a model, you will repeat the following procedures:\n",
        "        1. get one batch of features and labels\n",
        "        2. make a forward pass with the features to get predictions\n",
        "        3. calculate the loss with the predictions and target labels\n",
        "        4. run a backward pass from the loss function to get the gradients\n",
        "        5. apply the optimizer step to update the model paramters\n",
        "        '''\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "    \n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            self.optimizer.zero_grad()\n",
        "                \n",
        "            # Help from posted Github example:\n",
        "            # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        "            prediction = self.model(batch.text).squeeze(1)\n",
        "            loss = self.loss_fn(prediction, batch.label)\n",
        "            acc = binary_accuracy(prediction, batch.label)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "    \n",
        "    def train_model(self, train_iterator, dev_iterator):\n",
        "        \"\"\"\n",
        "        This function processes the entire training set for multiple epochs.\n",
        "        After each training epoch, you will evaluate your model on the DEV set. \n",
        "        The best performing model on the DEV set shall be saved to best_model\n",
        "        \"\"\"  \n",
        "        dev_accs = [0.]\n",
        "        for epoch in range(15): # incrase epochs from 5 to 20\n",
        "            self.train_epoch(train_iterator)\n",
        "            dev_acc = self.evaluate(dev_iterator)\n",
        "            print(\"dev acc: {}\".format(dev_acc[1]), \"dev loss:{}\".format(dev_acc[0]))\n",
        "            if dev_acc[1] > max(dev_accs):\n",
        "                best_model = copy.deepcopy(self)\n",
        "            dev_accs.append(dev_acc[1])\n",
        "        return best_model.model\n",
        "                \n",
        "    def evaluate(self, iterator):\n",
        "        '''\n",
        "        This function evaluate the data with the current model.\n",
        "        1. make a forward pass with the features to get predictions\n",
        "        2. calculate the loss with the predictions and target labels\n",
        "        3. Use the binary accuracy function to compute the accuracy\n",
        "        '''\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "    \n",
        "        #model.eval()\n",
        "    \n",
        "        with torch.no_grad():\n",
        "    \n",
        "            for batch in iterator:\n",
        "\n",
        "                predictions = self.model(batch.text).squeeze(1)\n",
        "        \n",
        "                loss = self.loss_fn(predictions, batch.label)\n",
        "        \n",
        "                acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "        \n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YeyrD4wKiNa6",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "#You can try many different embedding dimensions. Common values are 20, 32, 64, 100, 128, 512\n",
        "EMBEDDING_DIM = 512\n",
        "OUTPUT_DIM = 1\n",
        "#Get the index of the pad token using the stoi function\n",
        "\n",
        "# From Github\n",
        "# https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/5%20-%20Multi-class%20Sentiment%20Analysis.ipynb\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "\n",
        "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3lOyhJl0iNa9",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "tm =Training_module(model)\n",
        "\n",
        "#Traing the model\n",
        "best_model = tm.train_model(train_iterator, valid_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9-DaC-kWiNbB",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "tm.model = best_model\n",
        "test_loss, test_acc = tm.evaluate(test_iterator)\n",
        "#Accuracy on the best data. Should be possible to get accuracy around 80-85%\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LBjB6jy9iNbH"
      },
      "source": [
        "Now spend some time to tune your models. At least try the following: \n",
        "\n",
        "- try another optimizer\n",
        "- change the number of epochs to train\n",
        "- Add a dropout layer to the embedding layer\n",
        "- Try different embedding sizes\n",
        "\n",
        "Report your results and analysis in the writeup. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s5VFreO7iNbH"
      },
      "source": [
        "Compute squared norms of the word vectors. List 10 words with highest norm and 10 with lowest norms. Explain your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTEQgTROiAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the weights from the linear layer \n",
        "weights = tm.model.linear.weight\n",
        "embs = tm.model.emb\n",
        "\n",
        "l2_word_vector = torch.matmul(embs.weight, weights.T)\n",
        "\n",
        "# Highest\n",
        "word_values, word_indices = torch.topk(input=l2_word_vector.T, k=10, largest=True) \n",
        "\n",
        "words = word_indices.tolist()[0]\n",
        "print(\"\\nHighest Norm Words\")\n",
        "print(f\"Indices: {words}\")\n",
        "\n",
        "i = 1\n",
        "for word in words: \n",
        "  print(f'#{i} {TEXT.vocab.itos[word]}')\n",
        "  i += 1\n",
        "\n",
        "# Lowest\n",
        "word_values, word_indices = torch.topk(input=l2_word_vector.T, k=10, largest=False)  \n",
        "\n",
        "\n",
        "words = word_indices.tolist()[0]\n",
        "print(\"\\nLowest Norm Words\")\n",
        "print(f\"Indices: {words}\")\n",
        "\n",
        "i = 1\n",
        "for word in words: \n",
        "  print(f'#{i} {TEXT.vocab.itos[word]}')\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "rbDJaNISiNbI"
      },
      "source": [
        "## Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a0ynpR4ziNbJ"
      },
      "source": [
        "Use the code given below to upload your training, test and dev dataset if you're using Google collab for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLviIWMMinok",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h9hlOczKlcJK",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "USE_CUDA = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Elc6h-brAnn",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 650\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "LOG_FILE = \"language-model.log\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BhaIokCsiNbM",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "TEXT = data.Field(lower=True)\n",
        "train, val, test = datasets.LanguageModelingDataset.splits(path=\".\", \n",
        "    train=\"text8.train.txt\", validation=\"text8.dev.txt\", test=\"text8.test.txt\", text_field=TEXT)\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))\n",
        "\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iter, val_iter, test_iter = data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device= device, bptt_len=32, repeat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0LMra11ziNbS"
      },
      "source": [
        "How to get text and target using torchtext. The learning goal of our language model is to predict the next word using the previous words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mExWtvGtiNbV",
        "colab": {}
      },
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it)\n",
        "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1].data]))\n",
        "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1].data]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6_YsgnjhiNbZ"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kN4Jd3WTiNbb",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a recurrent module, and a decoder.\n",
        "        Feel free to add more methods into this class if necessary. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        ''' Create the layers of your model. You will need the following layers:\n",
        "            - embedding layer\n",
        "            - recurrent neural network layer (LSTM, GRU)\n",
        "            - linear decoding layer to map from hidden vector to the vocabulary\n",
        "            - optionally, add the dropout layer. The dropout layer can be put after \n",
        "                the embedding layer or/and after the RNN layer, or other places. \n",
        "            - optionally, initialize your model parameters. Look for papers/blogs \n",
        "                online for good parameter initialization methods. \n",
        "            Please read the documentation for how to build LSTM with PyTorch. \n",
        "            https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM\n",
        "        '''\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ''' The forward layer. You will need to do the following:\n",
        "            - embed word index to word vectors\n",
        "            - run RNN\n",
        "            - linear layer to decode hidden vectors to output words\n",
        "        '''\n",
        "        \n",
        "        input = self.encoder(input) # encode\n",
        "        emb = self.drop(input) # embedings\n",
        "        out, hidden = self.rnn(emb, hidden) # get hidden vectors\n",
        "        out = self.drop(out) # drop out layer\n",
        "\n",
        "        out = self.decoder(out) # decode the output words\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, bsz, requires_grad=True):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),\n",
        "                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))\n",
        "        else:\n",
        "            return weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6dofhpgiNbf"
      },
      "source": [
        "Implement the evaluation script, return the loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "un1P3hLCiNbg",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    it = iter(data)\n",
        "    total_count = 0.\n",
        "    with torch.no_grad():\n",
        "        ''' Fill in your evaluation code here. The evaulation follows the same logic as training. \n",
        "        You might want to finish the training part first. \n",
        "        '''\n",
        "    \n",
        "        \n",
        "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
        "        for i, batch in enumerate(it):\n",
        "            data, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            with torch.no_grad():\n",
        "                output, hidden = model(data, hidden)\n",
        "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
        "            total_count += np.multiply(*data.size())\n",
        "            total_loss += loss.item()*np.multiply(*data.size())\n",
        "            \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPrhDIiDiNbi",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "# Remove this part\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "model = RNNModel(\"GRU\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()\n",
        "#Use cross entropy as your loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss() \n",
        "learning_rate = 0.001\n",
        "#Choose your favorite Adam's optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "val_losses = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    it = iter(train_iter)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    for i, batch in enumerate(it):\n",
        "        ''' The training code. You need to do the following:\n",
        "            - prepare the training tensors, including the tensors of the history words and the predicted words\n",
        "            - zero the model gradidents\n",
        "            - predict the next words using the model\n",
        "            - compute the cross entropy loss\n",
        "            - do back propagation \n",
        "            - clip the gradients as we are training RNN\n",
        "            - update the model with the gradidents\n",
        "            - optionally, print out the batch loss after every 1000 iterations. \n",
        "            - optionally, evaluate your model on DEV after every 10000 iterations and save it to best_model. \n",
        "        '''\n",
        "  \n",
        "        \n",
        "        data, target = batch.text, batch.target\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        out, hidden = model(data, hidden)\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        loss = loss_fn(out.view(-1, VOCAB_SIZE), target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        # apply gradient clipping to prevent the exploding gradient problem in RNN\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        \n",
        "        optimizer.step() # update the model\n",
        "\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
        "    \n",
        "        if i % 10000 == 0:\n",
        "            val_loss = evaluate(model, val_iter)\n",
        "            perplexity = 2**val_loss \n",
        "            with open(LOG_FILE, \"a\") as fout:\n",
        "                print(f\"epoch: {epoch}, iteration: {i}, perplexity: {perplexity}\")\n",
        "                fout.write(f\"epoch: {epoch}, iteration: {i}, perplexity: {perplexity}\\n\"  )\n",
        "                \n",
        "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
        "                print(\"best model, val loss: \", val_loss)\n",
        "                \n",
        "                # The following may not work on Colab.  Adapt it based on\n",
        "                # https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514\n",
        "                best_model = copy.deepcopy(model)\n",
        "                \n",
        "                with open(\"lm-best.th\", \"wb\") as fout:\n",
        "                    torch.save(best_model.state_dict(), fout)\n",
        "            else:\n",
        "                learning_rate /= 4.\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            val_losses.append(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6h_5rf7biNbn",
        "colab": {}
      },
      "source": [
        "val_loss = evaluate(best_model, val_iter)\n",
        "print(\"perplexity: \", 2**val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q00TgtX9iNbr"
      },
      "source": [
        "#### Use the best model to evaluate the test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vNKLdiVniNbt",
        "colab": {}
      },
      "source": [
        "test_loss = evaluate(best_model, test_iter)\n",
        "print(\"perplexity: \", 2**test_loss )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ex6bSf4LiNbv"
      },
      "source": [
        "Generate some sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G4EJ8B7piNbw",
        "colab": {}
      },
      "source": [
        "hidden = best_model.init_hidden(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
        "words = []\n",
        "for i in range(100):\n",
        "    output, hidden = best_model(input, hidden)\n",
        "    word_weights = output.squeeze().exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "    word = TEXT.vocab.itos[word_idx]\n",
        "    words.append(word)\n",
        "print(\" \".join(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiGFSzREaGlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Greedy Decoding\n",
        "hidden = best_model.init_hidden(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
        "words = []\n",
        "\n",
        "output, hidden = best_model(input, hidden)\n",
        "word_weights = output.squeeze().exp().cpu()\n",
        "val, word_idx = torch.topk(input=word_weights, k=100)\n",
        "word_idx_list = word_idx.data\n",
        "\n",
        "for idx in word_idx: \n",
        "  input.fill_(idx)\n",
        "  word = TEXT.vocab.itos[idx]\n",
        "  words.append(word)\n",
        "print(\" \".join(words))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bndr7Oj8Vzwb",
        "colab_type": "text"
      },
      "source": [
        "# Best Results\n",
        "\n",
        "## Using 5 epochs\n",
        "\n",
        "\n",
        "```\n",
        "epoch 0 iter 0 loss 10.82283878326416\n",
        "epoch: 0, iteration: 0, perplexity: 1757.6632350478521\n",
        "best model, val loss:  10.779442963978829\n",
        "epoch 0 iter 1000 loss 6.506584167480469\n",
        "epoch 0 iter 2000 loss 6.348207473754883\n",
        "epoch 0 iter 3000 loss 6.102053642272949\n",
        "epoch 0 iter 4000 loss 5.419452667236328\n",
        "epoch 0 iter 5000 loss 5.919216156005859\n",
        "epoch 0 iter 6000 loss 5.8455810546875\n",
        "epoch 0 iter 7000 loss 5.6072678565979\n",
        "epoch 0 iter 8000 loss 5.785408020019531\n",
        "epoch 0 iter 9000 loss 5.4222731590271\n",
        "epoch 0 iter 10000 loss 5.598560333251953\n",
        "epoch: 0, iteration: 10000, perplexity: 38.53165289463282\n",
        "best model, val loss:  5.267972169588238\n",
        "epoch 0 iter 11000 loss 5.6927289962768555\n",
        "epoch 0 iter 12000 loss 5.678534984588623\n",
        "epoch 0 iter 13000 loss 5.372802734375\n",
        "epoch 0 iter 14000 loss 5.291945934295654\n",
        "epoch 1 iter 0 loss 5.660902976989746\n",
        "epoch: 1, iteration: 0, perplexity: 34.76176337103176\n",
        "best model, val loss:  5.11942936294636\n",
        "epoch 1 iter 1000 loss 5.579431533813477\n",
        "epoch 1 iter 2000 loss 5.623998641967773\n",
        "epoch 1 iter 3000 loss 5.434945106506348\n",
        "epoch 1 iter 4000 loss 4.951798915863037\n",
        "epoch 1 iter 5000 loss 5.430264472961426\n",
        "epoch 1 iter 6000 loss 5.405932426452637\n",
        "epoch 1 iter 7000 loss 5.3224263191223145\n",
        "epoch 1 iter 8000 loss 5.420109748840332\n",
        "epoch 1 iter 9000 loss 5.1500983238220215\n",
        "epoch 1 iter 10000 loss 5.285759449005127\n",
        "epoch: 1, iteration: 10000, perplexity: 30.813975719112218\n",
        "best model, val loss:  4.945512930468414\n",
        "epoch 1 iter 11000 loss 5.3877716064453125\n",
        "epoch 1 iter 12000 loss 5.361586093902588\n",
        "epoch 1 iter 13000 loss 5.165499687194824\n",
        "epoch 1 iter 14000 loss 5.0832414627075195\n",
        "epoch 2 iter 0 loss 5.469143390655518\n",
        "epoch: 2, iteration: 0, perplexity: 29.996484974268075\n",
        "best model, val loss:  4.90672154869852\n",
        "epoch 2 iter 1000 loss 5.311384677886963\n",
        "epoch 2 iter 2000 loss 5.427046775817871\n",
        "epoch 2 iter 3000 loss 5.298762321472168\n",
        "epoch 2 iter 4000 loss 4.734654426574707\n",
        "epoch 2 iter 5000 loss 5.279272079467773\n",
        "epoch 2 iter 6000 loss 5.2258148193359375\n",
        "epoch 2 iter 7000 loss 5.187764644622803\n",
        "epoch 2 iter 8000 loss 5.244477272033691\n",
        "epoch 2 iter 9000 loss 5.050817012786865\n",
        "epoch 2 iter 10000 loss 5.070919036865234\n",
        "epoch: 2, iteration: 10000, perplexity: 28.432046544494217\n",
        "best model, val loss:  4.829446043122923\n",
        "epoch 2 iter 11000 loss 5.249824047088623\n",
        "epoch 2 iter 12000 loss 5.231677532196045\n",
        "epoch 2 iter 13000 loss 5.055905342102051\n",
        "epoch 2 iter 14000 loss 5.032273292541504\n",
        "epoch 3 iter 0 loss 5.37913179397583\n",
        "epoch: 3, iteration: 0, perplexity: 28.1427962437409\n",
        "best model, val loss:  4.814693775507217\n",
        "epoch 3 iter 1000 loss 5.192203998565674\n",
        "epoch 3 iter 2000 loss 5.328357696533203\n",
        "epoch 3 iter 3000 loss 5.265546798706055\n",
        "epoch 3 iter 4000 loss 4.672381401062012\n",
        "epoch 3 iter 5000 loss 5.262495994567871\n",
        "epoch 3 iter 6000 loss 5.129266262054443\n",
        "epoch 3 iter 7000 loss 5.055858135223389\n",
        "epoch 3 iter 8000 loss 5.183602333068848\n",
        "epoch 3 iter 9000 loss 4.950128555297852\n",
        "epoch 3 iter 10000 loss 5.034492015838623\n",
        "epoch: 3, iteration: 10000, perplexity: 27.25127013620251\n",
        "best model, val loss:  4.768251567951126\n",
        "epoch 3 iter 11000 loss 5.181535720825195\n",
        "epoch 3 iter 12000 loss 5.18128776550293\n",
        "epoch 3 iter 13000 loss 5.033013343811035\n",
        "epoch 3 iter 14000 loss 4.940471172332764\n",
        "```\n",
        "\n",
        "\n",
        "## Results\n",
        "\n",
        "Best Training perplexity:  27.25127013620251\n",
        "\n",
        "Test perplexity:  32.441465873566145\n",
        "\n",
        "## Sentences\n",
        "\n",
        "### Multinomial\n",
        "\n",
        "s jaws was regularly grounded for scavengers on the prevention of amber its transfer head that had to bite buster his steam orbiter to cure anyone was torso until danger of and stroke dragged from my mouth to practice <unk> remarked after the poison cared for the sketch of the sea alters their health of struggle that would return at a battle the short attempt for connally client robert common earlier evidence on the voyage of miss lamia may have never had a number of swords than even before balloons or behave less would isolate this object effects of the\n",
        "\n",
        "\n",
        "bergman walpole college of oz costly kin <unk> <unk> american football team name play for translations page knee pda progress called a good permission that applies stolen by sixty have of typing and quality reference australian history several videos from a series entitled development mailing list readings but managed for society external links entry and <unk> sites to help adobe <unk> a for discovery includes malaria coverage com int beta ratio segregated of drinking statistical travel and knowledge of carbon dioxide energy energy element <unk> john walton dk from unintended life to disease with a spotlight at home each of\n",
        "\n",
        "pronounced the <unk> k which covers the first punic forces because lunar saharan orders visited almost all the <unk> became the world transportation act it can fallen into the united states polish architecture the element of the structures w jorge wilmington was patented on july two six one nine five eight as well as the two zero zero five nobel prize winners encounter he asserts that there has failed to settle off of surface contained and gaining success from the investments so he is stupid for its use at the national sit in princeton metropolitan new york city famous film\n",
        "\n",
        "### Greedy Decoder\n",
        "\n",
        "<unk> the and of in one s a is for on as two that which to also was or are by with three it who i other this at from an e but see have has four de can where five such were so external will m zero eight most including six first seven often about while more may nine all however called some new now used like based these g main not he no although many another do there v university d la they image high o r com sometimes below thus his ii found later state references p\n",
        "\n",
        "\n",
        "and conference university encyclopedia period in law fiction <unk> community the assembly of order era edition center council one series time is article review was convention s system society congress institute tradition park league history college book school events text view wars sites movement on crisis meeting model library site or day equation as description rule definition research with version project organization discussion records revolution accounts awards age church agreement see but entry interpretation reports hall campaign style map process cycle foundation analysis studies states film account office work vision opinion comics writers historians atlas calendar issues institution academy works\n",
        "\n",
        "microsystems iv <unk> s devils iii ii i and brown smith yat jones bonds one phi v hunt lewis o ndez viper lia todd the adams tley e two vi loves byrd island g jackson h clark bonaparte massachusetts genesis dioxide theta rogers wyatt bay richards green coffey shall brien roses hare robertson stewart jenkins clarke rays x mandela klan alpha bond shirt ashby springs anderson xi fairbanks wallace hamilton cells wolf else walks viii edwards monroe tree xii roberts is beta doyle johnson goes austen rouge w pigs grey atoms collins pi van epsilon lane fraternity mary blake t\n",
        "\n",
        "\n",
        "# Experimentation\n",
        "\n",
        "Increasing the number of epochs reduces the perplexity, but only 2 were used in the above after a few trials for time. \n",
        "\n",
        "Trying withGated Recurrent Unit (\"GRU\") was also an improvement. "
      ]
    }
  ]
}